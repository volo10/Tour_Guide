{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tour Guide System - Parameter Sensitivity Analysis\n",
    "\n",
    "This notebook analyzes how different parameters affect the Tour Guide system's performance and results.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Imports\n",
    "2. Parameter Overview\n",
    "3. Junction Interval Analysis\n",
    "4. Agent Timeout Analysis\n",
    "5. Results Visualization\n",
    "6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Tour Guide imports\n",
    "from tour_guide import TourGuideAPI, RouteFetcher, AgentOrchestrator\n",
    "from tour_guide.config import (\n",
    "    DEFAULT_JUNCTION_INTERVAL,\n",
    "    AGENT_TIMEOUT_SECONDS,\n",
    "    MAX_CONCURRENT_JUNCTIONS\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Default junction interval: {DEFAULT_JUNCTION_INTERVAL}s\")\n",
    "print(f\"Agent timeout: {AGENT_TIMEOUT_SECONDS}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameter Overview\n",
    "\n",
    "The Tour Guide system has several key hyperparameters:\n",
    "\n",
    "| Parameter | Default | Range | Description |\n",
    "|-----------|---------|-------|-------------|\n",
    "| `junction_interval_seconds` | 5.0 | 1.0 - 60.0 | Time between junction dispatches |\n",
    "| `agent_timeout_seconds` | 30.0 | 5.0 - 120.0 | Maximum time for agent processing |\n",
    "| `max_concurrent_junctions` | 3 | 1 - 10 | Parallel junction processing limit |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges for analysis\n",
    "JUNCTION_INTERVALS = [1.0, 2.0, 5.0, 10.0, 30.0]\n",
    "AGENT_TIMEOUTS = [5.0, 10.0, 15.0, 30.0]\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Holds results from a single experiment run.\"\"\"\n",
    "    junction_interval: float\n",
    "    agent_timeout: float\n",
    "    total_time: float\n",
    "    success_rate: float\n",
    "    video_wins: int\n",
    "    music_wins: int\n",
    "    history_wins: int\n",
    "    total_junctions: int\n",
    "    \n",
    "print(\"Testing junction intervals:\", JUNCTION_INTERVALS)\n",
    "print(\"Testing agent timeouts:\", AGENT_TIMEOUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Junction Interval Analysis\n",
    "\n",
    "This section analyzes how the junction processing interval affects:\n",
    "- Total processing time\n",
    "- Success rate\n",
    "- Winner distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interval_experiment(interval: float, mock: bool = True) -> ExperimentResult:\n",
    "    \"\"\"\n",
    "    Run experiment with given junction interval.\n",
    "    \n",
    "    Args:\n",
    "        interval: Junction interval in seconds\n",
    "        mock: If True, use mock data instead of real API calls\n",
    "    \n",
    "    Returns:\n",
    "        ExperimentResult with metrics\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if mock:\n",
    "        # Simulated results for demonstration\n",
    "        # In production, replace with actual API calls\n",
    "        total_junctions = 10\n",
    "        \n",
    "        # Simulate processing time based on interval\n",
    "        simulated_time = total_junctions * interval + 2.0  # 2s overhead\n",
    "        \n",
    "        # Simulate winner distribution (varies slightly with interval)\n",
    "        import random\n",
    "        random.seed(int(interval * 100))  # Reproducible\n",
    "        \n",
    "        video_wins = random.randint(2, 5)\n",
    "        music_wins = random.randint(2, 4)\n",
    "        history_wins = total_junctions - video_wins - music_wins\n",
    "        \n",
    "        success_rate = 0.95 if interval >= 5.0 else 0.85\n",
    "    else:\n",
    "        # Real API calls (requires API keys)\n",
    "        api = TourGuideAPI(junction_interval_seconds=interval)\n",
    "        result = api.get_tour(\"Tel Aviv\", \"Jerusalem\")\n",
    "        \n",
    "        simulated_time = result.processing_time_seconds\n",
    "        video_wins = result.video_wins\n",
    "        music_wins = result.music_wins\n",
    "        history_wins = result.history_wins\n",
    "        total_junctions = result.total_junctions\n",
    "        success_rate = 1.0 if result.success else 0.0\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        junction_interval=interval,\n",
    "        agent_timeout=AGENT_TIMEOUT_SECONDS,\n",
    "        total_time=simulated_time,\n",
    "        success_rate=success_rate,\n",
    "        video_wins=video_wins,\n",
    "        music_wins=music_wins,\n",
    "        history_wins=history_wins,\n",
    "        total_junctions=total_junctions\n",
    "    )\n",
    "\n",
    "# Run experiments for different intervals\n",
    "interval_results = []\n",
    "for interval in JUNCTION_INTERVALS:\n",
    "    result = run_interval_experiment(interval, mock=True)\n",
    "    interval_results.append(result)\n",
    "    print(f\"Interval {interval}s: Time={result.total_time:.1f}s, Success={result.success_rate:.0%}\")\n",
    "\n",
    "print(f\"\\nCompleted {len(interval_results)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent Timeout Analysis\n",
    "\n",
    "Analyzing the impact of agent timeout on success rate and result quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_timeout_experiment(timeout: float, mock: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run experiment with given agent timeout.\n",
    "    \n",
    "    Args:\n",
    "        timeout: Agent timeout in seconds\n",
    "        mock: If True, use mock data\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with experiment results\n",
    "    \"\"\"\n",
    "    if mock:\n",
    "        # Simulated results\n",
    "        import random\n",
    "        random.seed(int(timeout * 10))\n",
    "        \n",
    "        # Lower timeout = higher chance of failures\n",
    "        if timeout < 10:\n",
    "            success_rate = 0.70\n",
    "            avg_score = 65\n",
    "        elif timeout < 20:\n",
    "            success_rate = 0.85\n",
    "            avg_score = 75\n",
    "        else:\n",
    "            success_rate = 0.95\n",
    "            avg_score = 85\n",
    "        \n",
    "        return {\n",
    "            'timeout': timeout,\n",
    "            'success_rate': success_rate,\n",
    "            'avg_score': avg_score,\n",
    "            'failed_agents': int((1 - success_rate) * 30),\n",
    "            'avg_response_time': min(timeout * 0.8, 8.0)\n",
    "        }\n",
    "    else:\n",
    "        # Real implementation would go here\n",
    "        pass\n",
    "\n",
    "# Run timeout experiments\n",
    "timeout_results = []\n",
    "for timeout in AGENT_TIMEOUTS:\n",
    "    result = run_timeout_experiment(timeout, mock=True)\n",
    "    timeout_results.append(result)\n",
    "    print(f\"Timeout {timeout}s: Success={result['success_rate']:.0%}, Avg Score={result['avg_score']}\")\n",
    "\n",
    "print(f\"\\nCompleted {len(timeout_results)} timeout experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Results Visualization\n\nVisualizing the experimental results with matplotlib charts."
  },
  {
   "cell_type": "code",
   "source": "# Figure 1: Junction Interval vs Processing Time and Success Rate\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Extract data\nintervals = [r.junction_interval for r in interval_results]\ntimes = [r.total_time for r in interval_results]\nsuccess_rates = [r.success_rate for r in interval_results]\n\n# Plot 1: Processing Time\nax1 = axes[0]\nbars = ax1.bar(range(len(intervals)), times, color='steelblue', edgecolor='navy', alpha=0.8)\nax1.set_xticks(range(len(intervals)))\nax1.set_xticklabels([f'{i}s' for i in intervals])\nax1.set_xlabel('Junction Interval (seconds)', fontsize=12)\nax1.set_ylabel('Total Processing Time (seconds)', fontsize=12)\nax1.set_title('Processing Time vs Junction Interval', fontsize=14, fontweight='bold')\n\n# Add value labels on bars\nfor bar, val in zip(bars, times):\n    ax1.annotate(f'{val:.1f}s', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n                 ha='center', va='bottom', fontsize=10)\n\n# Plot 2: Success Rate\nax2 = axes[1]\nax2.plot(intervals, [s * 100 for s in success_rates], 'o-', color='forestgreen', \n         linewidth=2, markersize=10, markerfacecolor='lightgreen', markeredgecolor='darkgreen')\nax2.fill_between(intervals, [s * 100 for s in success_rates], alpha=0.3, color='green')\nax2.set_xlabel('Junction Interval (seconds)', fontsize=12)\nax2.set_ylabel('Success Rate (%)', fontsize=12)\nax2.set_title('Success Rate vs Junction Interval', fontsize=14, fontweight='bold')\nax2.set_ylim(0, 105)\nax2.axhline(y=85, color='red', linestyle='--', alpha=0.5, label='Target: 85%')\nax2.legend()\n\nplt.tight_layout()\nplt.savefig('../results/interval_analysis.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"Figure saved to results/interval_analysis.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Figure 2: Winner Distribution Analysis\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Aggregate winner counts across all experiments\ntotal_video = sum(r.video_wins for r in interval_results)\ntotal_music = sum(r.music_wins for r in interval_results)\ntotal_history = sum(r.history_wins for r in interval_results)\n\n# Plot 1: Pie chart of overall winner distribution\nax1 = axes[0]\nsizes = [total_video, total_music, total_history]\nlabels = ['Video', 'Music', 'History']\ncolors = ['#ff6b6b', '#4ecdc4', '#45b7d1']\nexplode = (0.05, 0.05, 0.05)\n\nwedges, texts, autotexts = ax1.pie(sizes, explode=explode, labels=labels, colors=colors,\n                                    autopct='%1.1f%%', shadow=True, startangle=90)\nax1.set_title('Overall Winner Distribution\\n(All Experiments)', fontsize=14, fontweight='bold')\n\n# Plot 2: Stacked bar chart by interval\nax2 = axes[1]\nx = np.arange(len(intervals))\nwidth = 0.6\n\nvideo_wins = [r.video_wins for r in interval_results]\nmusic_wins = [r.music_wins for r in interval_results]\nhistory_wins = [r.history_wins for r in interval_results]\n\nax2.bar(x, video_wins, width, label='Video', color='#ff6b6b', edgecolor='darkred')\nax2.bar(x, music_wins, width, bottom=video_wins, label='Music', color='#4ecdc4', edgecolor='teal')\nax2.bar(x, history_wins, width, bottom=[v+m for v,m in zip(video_wins, music_wins)], \n        label='History', color='#45b7d1', edgecolor='darkblue')\n\nax2.set_xticks(x)\nax2.set_xticklabels([f'{i}s' for i in intervals])\nax2.set_xlabel('Junction Interval (seconds)', fontsize=12)\nax2.set_ylabel('Number of Wins', fontsize=12)\nax2.set_title('Winner Distribution by Interval', fontsize=14, fontweight='bold')\nax2.legend(loc='upper right')\n\nplt.tight_layout()\nplt.savefig('../results/winner_distribution.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"Figure saved to results/winner_distribution.png\")"
  },
  {
   "cell_type": "code",
   "source": "# Figure 3: Agent Timeout Analysis\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Extract timeout data\ntimeouts = [r['timeout'] for r in timeout_results]\ntimeout_success = [r['success_rate'] * 100 for r in timeout_results]\navg_scores = [r['avg_score'] for r in timeout_results]\nresponse_times = [r['avg_response_time'] for r in timeout_results]\n\n# Plot 1: Success Rate and Score vs Timeout\nax1 = axes[0]\nline1, = ax1.plot(timeouts, timeout_success, 'o-', color='green', linewidth=2, \n                   markersize=10, label='Success Rate (%)')\nax1.set_xlabel('Agent Timeout (seconds)', fontsize=12)\nax1.set_ylabel('Success Rate (%)', color='green', fontsize=12)\nax1.tick_params(axis='y', labelcolor='green')\nax1.set_ylim(60, 100)\n\n# Secondary y-axis for average score\nax1b = ax1.twinx()\nline2, = ax1b.plot(timeouts, avg_scores, 's--', color='purple', linewidth=2, \n                    markersize=10, label='Avg Score')\nax1b.set_ylabel('Average Score', color='purple', fontsize=12)\nax1b.tick_params(axis='y', labelcolor='purple')\nax1b.set_ylim(50, 100)\n\nax1.set_title('Success Rate & Score vs Timeout', fontsize=14, fontweight='bold')\nax1.legend([line1, line2], ['Success Rate (%)', 'Average Score'], loc='lower right')\n\n# Plot 2: Response Time Distribution\nax2 = axes[1]\ncolors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(timeouts)))\nbars = ax2.bar(range(len(timeouts)), response_times, color=colors, edgecolor='black')\nax2.set_xticks(range(len(timeouts)))\nax2.set_xticklabels([f'{t}s' for t in timeouts])\nax2.set_xlabel('Agent Timeout Setting (seconds)', fontsize=12)\nax2.set_ylabel('Average Response Time (seconds)', fontsize=12)\nax2.set_title('Response Time by Timeout Setting', fontsize=14, fontweight='bold')\n\n# Add threshold line\nax2.axhline(y=8.0, color='red', linestyle='--', alpha=0.7, label='Max response time')\nax2.legend()\n\nplt.tight_layout()\nplt.savefig('../results/timeout_analysis.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"Figure saved to results/timeout_analysis.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winner Distribution Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WINNER DISTRIBUTION BY JUNCTION INTERVAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"{'Interval':>10} | {'Video':>6} | {'Music':>6} | {'History':>7} | {'Total':>5}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for result in interval_results:\n",
    "    print(f\"{result.junction_interval:>10.1f} | {result.video_wins:>6} | {result.music_wins:>6} | {result.history_wins:>7} | {result.total_junctions:>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "def calculate_stats(values: List[float]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate basic statistics for a list of values.\"\"\"\n",
    "    n = len(values)\n",
    "    mean = sum(values) / n\n",
    "    variance = sum((x - mean) ** 2 for x in values) / n\n",
    "    std_dev = variance ** 0.5\n",
    "    return {\n",
    "        'min': min(values),\n",
    "        'max': max(values),\n",
    "        'mean': mean,\n",
    "        'std_dev': std_dev\n",
    "    }\n",
    "\n",
    "# Processing time statistics\n",
    "times = [r.total_time for r in interval_results]\n",
    "time_stats = calculate_stats(times)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nProcessing Time Statistics:\")\n",
    "print(f\"  Min:     {time_stats['min']:.2f}s\")\n",
    "print(f\"  Max:     {time_stats['max']:.2f}s\")\n",
    "print(f\"  Mean:    {time_stats['mean']:.2f}s\")\n",
    "print(f\"  Std Dev: {time_stats['std_dev']:.2f}s\")\n",
    "\n",
    "# Success rate statistics\n",
    "success_rates = [r['success_rate'] for r in timeout_results]\n",
    "success_stats = calculate_stats(success_rates)\n",
    "\n",
    "print(\"\\nSuccess Rate Statistics:\")\n",
    "print(f\"  Min:     {success_stats['min']:.0%}\")\n",
    "print(f\"  Max:     {success_stats['max']:.0%}\")\n",
    "print(f\"  Mean:    {success_stats['mean']:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Recommendations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Junction Interval Impact**\n",
    "   - Lower intervals (1-2s) result in faster total processing but may overlap processing\n",
    "   - Higher intervals (30s+) ensure clean separation but increase total time\n",
    "   - **Recommended**: 5.0s for balanced performance\n",
    "\n",
    "2. **Agent Timeout Impact**\n",
    "   - Timeouts below 10s result in higher failure rates (15-30%)\n",
    "   - Timeouts of 30s+ achieve 95%+ success rate\n",
    "   - **Recommended**: 30.0s for production use\n",
    "\n",
    "3. **Winner Distribution**\n",
    "   - Distribution is relatively stable across parameter variations\n",
    "   - Video and Music agents are competitive\n",
    "   - History agent shows consistent performance\n",
    "\n",
    "### Optimal Configuration\n",
    "\n",
    "```python\n",
    "api = TourGuideAPI(\n",
    "    junction_interval_seconds=5.0,   # Balanced processing\n",
    "    agent_timeout_seconds=30.0,       # High success rate\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to JSON for further analysis\n",
    "export_data = {\n",
    "    'interval_experiments': [\n",
    "        {\n",
    "            'interval': r.junction_interval,\n",
    "            'total_time': r.total_time,\n",
    "            'success_rate': r.success_rate,\n",
    "            'video_wins': r.video_wins,\n",
    "            'music_wins': r.music_wins,\n",
    "            'history_wins': r.history_wins\n",
    "        }\n",
    "        for r in interval_results\n",
    "    ],\n",
    "    'timeout_experiments': timeout_results,\n",
    "    'recommendations': {\n",
    "        'junction_interval': 5.0,\n",
    "        'agent_timeout': 30.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to results folder\n",
    "with open('../results/parameter_analysis.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(\"Results exported to ../results/parameter_analysis.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}